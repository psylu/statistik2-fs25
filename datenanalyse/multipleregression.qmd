# Multiple Regression {#sec-multiple-regression}

```{r}
#| include: false
# Set working directory of R
knitr::opts_knit$set(root.dir = '../')
```

## Anwendungsbereich

Die multiple Regression wird zur Vorhersage einer metrischen Variable durch mehrere metrische Variablen verwendet. 

Der Zusammenhang wird als Einfluss mehrerer erklärenden Variablen^[Prädiktorvariablen, unabhängigen Variablen, Regressoren] (_predictor variables_) auf eine zu erklärende Variable^[Kriteriumsvariablen, abhhängigen Variablen, Regressand] (_outcome/response variable_) quantifiziert.
Dabei ist wichtig, dass mit Zusammenhang nicht zwingend ein Kausalzusammenhang gemeint ist. 
Kausalzusammenhänge müssen zusätzlich theoretisch begründet und methodisch implementiert (z.B. durch ein experimentelles Design) sein. 

Die multiple Regression modelliert den Zusammenhang von Variablen mit einer linearen Funktionen. 
Hierbei werden die Werte der zu erklärenden Variablen $y$ als Funktion der erklärenden Variable $x$ modelliert. 
Berechnet wird ein Achsenabschnitt (_intercept_) und ein Regressionskoeffizient für jeden Prädiktor. 


Diese Methode ermöglicht das Untersuchen von Fragestellungen, die mehrere Einflussfaktoren einbeziehen: 

- Beschreiben von Merkmalsunterschieden/-zusammenhängen ohne Wirkrichtung bzw. ohne kausalen Zusammenhang
- Beschreiben/Feststellen von kausalen Zusammenhängen (bei theoretischer Begründung und methodischer Implementation/experimenteller Studie)
- Prognose bzw. Vorhersage/Prädiktion von Merkmalsausprägungen für neue Datenpunkte
- Prädiktion von neuen Werten


__Typische Fragestellungen__:

- *Wie wirkt sich Training in einer Aufgabe und Schlafdauer in der vorherigen Nacht auf die Reaktionszeit aus?*

- *Beeinflussen Berufserfahrung und Weiterbildungstage das Jahreseinkommen?*

- *Steigt das Risiko einer psychischen Erkrankung mit zunehmendem Alter und körperlichen Erkrankungen?*

- *Wie gross ist das Risiko einer Erkrankung bei vorliegendem Testscore und Alter?*


<!-- __Beispiel einer multiplen Regression__:  -->

## Voraussetzungen

| Voraussetzung | Beschreibung | Vorgehen bei Verletzung |
|---|---|---|
| Abhängige Variable | eine abhängige Variable mit metrischem Skalenniveau | bei kategorialer Variable: logistische Regression. |
| Unabhängige Variable | eine unabhängige Variable mit metrischem Skalenniveau | Verwenden eines anderen statistischen Verfahrens (z.B. _t_-Test, _ANOVA_) oder Kodierung der UV durch Kodiervariablen (Dummy-Codierung) |
| Linearität | Linearer Zusammenhang zwischen den beiden Variablen. Die Linearitätsannahme kann durch die visuelle Inspektion des Punktediagramms (_Scatterplot_) überprüft werden. | bei nichtlinearem Zusammenhang zwischen den Variablen: Variablen transformieren (z.B. Logarithmierung) oder Anwenden von Multipler Regressionsanalyse zur Analyse nichtlinearer Zusammenhänge |
| Varianzhomogenität / Homoskedastizität | Die Abweichung von $y$ von der Regressionsgeraden sollte über verschiedene Werte der Prädiktorvariablen $x$ ähnlich sein und nimmt nicht signifikant zu oder ab. | Transformation von $y$, alternative Schätzmethoden oder Bootstrapping |
| Normalverteilung der Residuen | Ob die Residuen normalverteilt sind, kann mit einem Q-Q-Plot und dem Shapiro-Wilk-Test überprüft werden. | Transformation von $y$, alternative Schätzmethoden oder Bootstrapping |
| Unabhängigkeit der Fehler| Die Stichprobe muss so erhoben werden, dass die Merkmalsträger voneinander unabhängig sind. | Hierarchisch lineares Modell, dass die Abhängigkeit innerhalb der Datenstruktur berücksichtigt. |

: __Voraussetzungen multiple Regression__, Überprüfung und mögliche Alternativen bei Verletzung der Annahmen {.hover}

<br>

<aside> Auf alternative statistische Verfahren bei Verletzung der Annahmen (z.B. Transformationen oder _Multiple Regressionsanalyse für nichtlineare Zusammenhänge_) wird in diesem Semester nicht eingegangen. </aside>
                                                                                          
::: {.callout-note title="Metrische Prädiktoren"}
In diesem Kapitel werden Modelle für zwei metrische Prädiktoren behandelt. Multiple Regressionen können aber auch mit mehr als zwei Prädiktoren berechnet werden.

Im Kapitel [Allgemeines lineares Modell](alm.qmd) wird darauf eingegangen, dass Regressionen auch mit kategorialen Prädiktoren verwendet berechnet werden können.
:::
                                                                                     

## Grundkonzepte und Vorgehen

![Allgemeiner Workflow der Datenanalyse](../imgs/flowchart_analysis.png)

### Vorverarbeitung

#### __Daten einlesen, vorverarbeiten und visualisieren__

Zuerst müssen die Daten eingelesen und je nach Bedarf vorverarbeitet werden.

Daten für eine einfache lineare Regression können mit einem Punktediagramm (_scatterplot_) diagnostisch visualisiert werden. 
Üblicherweise (aber nicht zwingend) wird die Prädiktorvariable (unabhängige Variable) auf der $x$-Achse und Kriteriumsvariable (abhängige Variable) auf der $y$-Achse abgetragen.

Scatterplots eignen sich besonders gut, um folgende Probleme zu identifizieren: 

- Schlechte Datenqualität durch fehlende Werte
- Verzerrung der Resultate durch Extremwerte (Outlier)
- Nichtlinearer Zusammenhang der beiden Variablen

#### __Voraussetzungen überprüfen__

__Vor dem Berechnen der Regressionsanalyse__

- __Skalenniveau der beiden Variablen__: Beide Variablen sollten metrisch sein.^[Die multiple Regression kann auch mit kategorialen Variablen berechnet werden. Dies wird in diesem Kapitel nicht behandelt, siehe [Allgemeines lineares Modell](alm.qmd) für mehr Informationen.] 

- __Linearitätsannahme__: Durch ein Punktediagramm bzw. _Scatterplot_ kann ein starker, nichtlinearer Zusammenhang der Variablen ausgeschlossen werden.
Die meisten Zusammenhänge in der Psychologie und in den Verhaltenswissenschaften sind nur annähernd linear.
Bei leichten Abweichungen kann trotzdem eine einfache lineare Regression ausgeführt werden.

![Punktediagramme für Datensätze mit gegebener und nicht gegebener Linearität](../imgs/linearity.jpg)

### Analyse

#### __Regressionsgleichungen__

Die Anzahl der Regressionskoeffizienten entspricht der Anzahl der Prädiktorvariablen (plus allfälliger Interaktionsterme).

Folgende Regressionsgleichungen gelten für __zwei Prädiktoren__:

- Population: $y = \beta_0 + \beta_1 \cdot x_1 + \beta_2 \cdot x_2 + \varepsilon$

- Stichprobe: $y = b_0 + b_1 \cdot x_1 + b_2 \cdot x_2 + e$


Folgende Regressionsgleichungen gelten für __mehr als zwei Prädiktoren__:

- Population: $y = \beta_0 + \beta_1 \cdot x_1 + \beta_2 \cdot x_2 + ... + \beta_j \cdot x_j + ... + \beta_k \cdot x_k + \varepsilon$

- Stichprobe: $y = b_0 + b_1 \cdot x_1 + b_2 \cdot x_2 + ... + b_j \cdot x_j + ... + b_k \cdot x_k + e$

wobei $k$ die Summe aller Prädiktorvariablen bezeichnet, und $j$ eine Laufvariable ist, die bezeichnet, welche Variable gemeint ist.

<aside> In diesem Kapitel wird nur der "Spezial"-Fall einer Regressionsanalyse mit zwei Prädiktoren ($k = 2$ behandelt.</aside>

#### __Regressionsebene__

Bei der multiplen Regression mit zwei Prädiktoren wird für jeden Prädiktor ein Regressionskoeffizient geschätzt. 
Die Regressionskoeffizienten werden wie bei der einfachen Regression mittels OLS (_Ordinary Least Squares_) geschätzt: Nach dem Kleinste-Quadrate-Kriterium wird die Summe der Abweichungsquadrate minimiert.

Man kann sich die Schätzung zweier Regressionskoeffizienten vorstellen, wie 2 Geraden, die durch eine 3-dimensionale Punktewolke (Daten) gelegt werden. 
Die beiden Regressionsgeraden ergeben die Ränder der Regressionsebene.

![Zwei Regressionsgeraden ergeben eine Regressionsebene](../imgs/gif_regression_plane.gif)

Durch das Drehen des untenstehenden Plots kann erkannt werden, dass die Seite der Ebene für $x_1$ und $x_2$ eine Gerade bildet. Die interaktive Grafik zeigt, wie die beiden Regressionsgeraden zusammen eine Regressionsebene bilden. 
Der orange Punkt ist kein Datenpunkt, sondern markiert den Intercept $b_0$.

<br>

```{r}
#| echo: false
#| warning: false
#| message: false

# load package plotly for interactive 3d plots
library(plotly)

# set seed for reproducibility
set.seed(42)

#-------------------------------------------------------------------------------
# HERE YOU CAN CHANGE VALUES TO ALTER PLOT DATA

# change for example the intercept and the slopes
# when finished, run all the code below...

n <- 100 # number of datapoints
b_0 = 10 # intercept
b_1 = 0.5 # slope x1
b_2 = -0.5 # slope x2

mean_error <- 0
sd_errors <- 2 # make bigger for larger residuals

min_x = 0 # lower range of x
max_x = 20 # upper range of x

x1 <- runif(n, min = min_x, max = max_x)
x2 <- runif(n, min = min_x, max = max_x)

y <- b_0 + b_1 * x1 + b_2 * x2 + rnorm(n, mean = mean_error, sd = sd_errors)

#-------------------------------------------------------------------------------
#linear regression model
model <- lm(y ~ x1 + x2)

# data
x1_seq <- seq(min_x, max_x, length.out = 30)
x2_seq <- seq(min_x, max_x, length.out = 30)
grid <- expand.grid(x1 = x1_seq, x2 = x2_seq)
grid$y <- predict(model, newdata = grid)


#-------------------------------------------------------------------------------
# HERE YOU CAN CHANGE PARAMETERS TO ALTER PLOT 

# plot
plot_ly() |>
  add_markers(x = x1, y = x2, z = y,
              marker = list(size = 3),
              name = "Data Points",
              showlegend = FALSE) |>
  add_markers(x = 0, y = 0, z = model$coefficients[1], # plot intercept
              marker = list(size = 5),
              name = "Data Points",
              showlegend = FALSE) |>
  add_surface(x = ~x1_seq, 
              y = ~x2_seq, 
              z = matrix(grid$y, nrow = length(x1_seq), byrow = TRUE),
              opacity = 0.2,
              surfacecolor =  y,
              # colorscale = list(c(0, 1), c("lightblue", "black")),
              colorscale = "Cool",
              showscale = FALSE,
              name = "Plane") |>
  layout(scene = list(
    xaxis = list(title = "x1"),
    yaxis = list(title = "x2"),
    zaxis = list(title = "y")),
    title = "Linear Regression Plane")
#-------------------------------------------------------------------------------
```

<aside> Im _R-Skript_ [`regressionplane_3dmodel.R`](../data/regressionplane_3dmodel.R) können Sie die Werte für $b_0$, $x_1$, $x_2$ und $e$ verändern, und sich die angepasste Regressionsebene anschauen. Im _R-Skript_ [`regressionplane_3dmodel_tibble.R`](../data/regressionplane_3dmodel_tibble.R) können Sie eigene Werte für $x_1$, $x_2$, und $y$ definieren. </aside>


::: {.callout-tip title="Welche Werte haben $b_0$, $b_1$ und $b_2$?" collapse="true"}

- $b_0 = 10$
- $b_1 = 0.5$
- $b_2 = -0.5$

:::


#### __Bestimmung der Regressionskoeffizienten $b_1$__

Der Regressionskoeffizient bzw. die Steigung (_slope_) bei zwei Prädiktorvariablen wird mit folgender Gleichung bestimmt:

$b_1 = b_{1s} \cdot \frac{s_y}{s_{x_1}}$

wobei 

$b_{1s} = \frac{r_{yx_1}-r_{yx_2} \cdot r_{x_1x_2}}{1 - r^2_{x_1x_2}}$


wobei 

- $r_{yx_j}$: Produkt-Moment-Korrelation von $y$ und $x_j$
- $s_y$: Standardabweichung von $y$
- $s_{x_j}$: Standardabweichung von $x_j$

Dies bedeutet, dass die Korrelation der Variablen $x_1$ und $x_2$ stark Einfluss auf den Regressionskoeffizienten nimmt.

Wenn $x_1$ und $x_2$ nicht korrelieren, $r_{x_1x_2} = 0$, dann verändert sich der Regressionskoeffizient der Variablen $x1$ nicht zwischen einer einfachen Regression (mit nur Prädiktor $x_1$) und einer multiplen Regression (mit Prädiktoren $x_1$ und $x_2$).

Wenn hingegen die Korrelation zwischen $x_1$ und $x_2$ nicht null ist, $r_{x_1x_2} \neq 0$, dann entspricht ein Regressionskoeffizient (bei zwei Prädiktoren) der Steigung der Regressionsgeraden, wenn die Ausprägungen auf der zweiten Prädiktorvariablen konstant gehalten wird. 
Mit der multiplen Regressionsanalyse wird der Einfluss einer Prädiktorvariablen auf die abhängige Variable bei Konstanthaltung aller anderer Prädiktorvariablen untersucht.


#### __Bestimmung der Achsenabschnitts $b_0$__

Der Achsenabschnitt $b_0$ entspricht dem Wert von $y$, wenn $x_1$ und $x_2$ beide 0 sind.

Berechnet wird er wie folgt:

$b_0 = \bar{y} - b_1 \cdot \bar{x}_1 - b_2 \cdot \bar{x}_2$

![Achsenabschnitt: Der orange Punkt entspricht dem Achsenabschnitt. Er liegt dort, wo $x_1 = 0$ und $x_2 = 0$.](../imgs/intercept.png){width=50%}

#### __Determinations- und Indeterminationskoeffizient__

Der multiple Determinationskoeffizient $R^2$ entspricht der Wurzel aus der multiplen Korrelation ($R$). 
Er kann einen Wert zwischen 0 und 1 annehmen. 
Für Werte zwischen 0 und 1 gilt, dass $c \cdot 100%$ der Varianz in $y$ durch Variation in den Prädiktorvariablen erklärt wird, also zurückgeführt werden kann.
Ein Determinationskoeffizient von $0.45$ bedeutet also, dass 45% der Varianz in $y$ durch Variation in den Prädiktorvariablen erklärt werden kann.

Determinationskoeffizient: $R^2 = \frac{s^2_\hat{y}}{s^2_y}$

Indeterminationskoeffizient: $1- R^2 = \frac{s^2_e}{s^2_y}$

#### __Teststatistik berechnen__

Für die statistische Absicherung der multiplen Regression gegen $0$ können folgende Teststatistiken berechnet werden:

__Gesamtmodell__

$H_0: \beta_1 = ... = \beta_j = ... = \beta_k = 0$

$H_1:$ mindestens ein $\beta_j \neq 0$

Um zu überprüfen, ob die Prädiktorvariablen insgesamt einen Beitrag zur Erklärung in Unterschieden der abhängigen Variablen erklären, kann die ein $F$-Test durchgeführt werden.

Prüfgrösse ist $F$ mit $df_1 = k$ und $df_2 = n-k-1$ Freiheitsgraden. 
Vereinfacht kann gesagt werden:

$F = \frac{MQSR}{MQSE}$


__Regressionskoeffizienten__

$H_0: \beta_j = \beta_{0j}$ wobei $\beta_0j$ das Regressionsgewicht unter der Nullhypothese ist

$H_1: \beta_j \neq \beta_{0j}$

Gegen $0$ (häufigster Fall) wäre das

$H_0: \beta_j = 0$
$H_1: \beta_j \neq 0$


Es können wie bei vorherigen Tests auch gerichtete Hypothesen formuliert werden.

Die Signifikanz einzelner RegressionskoeffizientenS, kann mit einer $t$-Statistik berechnet werden.

Prüfgrösse ist $t$ mit $df = n-k-1$.

$t = \frac{b_j - \beta_{0j}}{\hat{\sigma}_{B_j}}$

Ein Konfidenzintervall kann konstruiert werden mit

$[(b_j - t_{(1 - \frac{\alpha}{2}; df)} \cdot \hat{\sigma}_{B_j}) ; (b_j + t_{(1 - \frac{\alpha}{2}; df)} \cdot \hat{\sigma}_{B_j}) ]$

### Schlussfolgerungen

#### __Ergebnisse interpretieren und berichten__

> Multiple regression analysis was used to test if the personality traits significantly predicted participants' ratings of aggression. The results of the regression indicated the two predictors explained 35.8% of the variance ($R^2$ = .38, $F$(2,55) = 5.56, $p$ < .01). It was found that extraversion significantly predicted aggressive tendencies ($\beta$ = .56, $p$ < .001), as did agreeableness ($\beta$ = -.36, $p$ < .01).

## Anwendungsbeispiel in R


::: {.callout-note appearance="default" title="Beispiel: Schlaf und Konzentration" icon=false}

__Studienbeschrieb__: In der (fiktiven) Studie wurde in Jugendlichen der Einfluss von Schlafdauer und Sport-Training auf die Konzentrationsleistung mit einem neuropsychologischen Test (`concentration`: mögliche Scores von 0 bis 20) untersucht.

__Forschungsfrage__: Hängt die Dauer des Sportrainings und die nächtliche Schlafdauer mit der Konzentration am nächsten Tag zusammen?

- Die _unabhängige Variable_ `sleep` enthält Daten über die Schlafdauer (in Stunden) in der vorherigen Nacht.
- Die _unabhängige Variable_ `sport` enthält Daten über die Traingsdauer (in Stunden) am vorherigen Tag.
- Die _abhängige Variable_ `concentration` enthält Daten über die Leistung in einem neuropsychologischen Konzentrationstest. Die minimale Punktzahl beträgt `0`, die maximale Punktzahl beträgt `20`.

```{r}
#| echo: false
#| warning: false
#| message: false
library(tidyverse)
# Erstellen des Datensatzes
d_sport <- tibble(sleep = c(5, 5, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9, 10, 10, 11, 11, 12, 12),
            sport = c(0.5, 5.4, 2.3, 2.3, 0.2, 0.4, 3.4, 1.2, 1.9, 3.1, 2.0, 5.2, 0.5, 0.6, 1.1, 2.1, 1.1, 1.9),
            concentration = c(6, 10, 10, 12, 6, 12, 17, 12, 8, 16, 12, 16, 18, 8, 14, 10, 12, 18))
```

:::

Wenn man zwei einzelne Regressionen für die beiden Prädiktorvariablen rechnen würde ergäbe dies zwei Variablenpaare: `concentration ~ sleep` (blaue Datenpunkte) und `concentration ~ sport` (rote Datenpunkte).

```{r}
#| echo: false
#| warning: false
#| message: false
library(patchwork)
p1 <- d_sport |>
  ggplot(aes(x = sleep, y = concentration)) +
  geom_point(size = 3, alpha = 0.3, color = "blue") +
  geom_smooth(method = "lm", se = FALSE) +
  scale_x_continuous("Schlafdauer in Stunden",
                     breaks=c(0,1,2,3,4,5,6,7,8,9,10,11,12),
                     minor_breaks = NULL,
                     limits=c(0,12)) + 
  scale_y_continuous("Leistung in Konzentrationstest",
                     breaks=c(0,2,4,6,8,10,12,14,16,18,20), 
                     minor_breaks = NULL,
                     limits=c(0,20)) +
  theme_minimal()

p2 <- d_sport |>
  ggplot(aes(x = sport, y = concentration)) +
  geom_point(size = 3, alpha = 0.3, color = "red") +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  scale_x_continuous("Sportdauer in Stunden",
                     breaks=c(0,1,2,3,4,5,6,7,8,9,10,11,12),
                     minor_breaks = NULL,
                     limits=c(0,12)) + 
  scale_y_continuous("Leistung in Konzentrationstest",
                     breaks=c(0,2,4,6,8,10,12,14,16,18,20), 
                     minor_breaks = NULL,
                     limits=c(0,20)) +
  theme_minimal()

p1 / p2
```

Bei der multiplen Regression werden beide Prädiktorvariablen gleichzeitig in ein Modell einbezogen.

### Planung

#### __Statistische Hypothesen und Signifikanzniveau festlegen__

Das Alphaniveau wird auf $5 \%$ festgelegt. Da es sich um eine ungerichtete Hypothese handelt, wird zweiseitig getestet.

__Hypothesen für den Gesamttest:__

$H_0$: Die Schlafdauer und die Sportdauer haben keinen Einfluss auf die Leistung im Konzentrationstest. $\beta_{sleep} = \beta_{sport} = 0$

$H_1$: Die Schlafdauer und/oder die Sportdauer sagen die Leistung im Konzentrationstest vorher. $\beta_{sleep}$ und/oder $\beta_{sport} \neq 0$

__Hypothese für einen Regressionskoeffizienten:__

$H_0$: Die Sportdauer hat keinen Einfluss auf die Leistung im Konzentrationstest. $\beta_{sport} = 0$

$H_1$: Die Sportdauer sagt die Leistung im Konzentrationstest vorher. $\beta_{sport} \neq 0$

#### __Daten einlesen und vorverarbeiten__

::: {.callout-caution appearance="default" title="Hands-on: Beispieldatensatz Schlaf, Sport und Konzentration"}

Der Datensatz kann selber generiert werden mit folgendem Code:

```{r}
# Laden tidyverse
library(tidyverse)

# Erstellen des Datensatzes
d_sport <- tibble(sleep = c(5, 5, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9, 10, 10, 11, 11, 12, 12),
            sport = c(0.5, 5.4, 2.3, 2.3, 0.2, 0.4, 3.4, 1.2, 1.9, 3.1, 2.0, 5.2, 0.5, 0.6, 1.1, 2.1, 1.1, 1.9),
            concentration = c(6, 10, 10, 12, 6, 12, 17, 12, 8, 16, 12, 16, 18, 8, 14, 10, 12, 18))
```

_Tipp: Sie können dies auch in einer [*WebR*-Konsole](../appendix/webrconsole.qmd) tun._ 

:::

Für die Regressionsanalyse mit metrischen Variablen sollten die Variablenwerte im Zahlenformat, also beispielsweise als `integer` oder `double` gespeichert sein.

#### __Voraussetzungen überprüfen__

__Metrisch skalierte Variablen und Linearität__

```{r}
glimpse(d_sport)
```

__Linearität__

Im obigen Plot sind keine auffälligen nonlinearen Beziehungen zwischen der `concentration` und `sleep` bzw. `concentration` sichtbar.

Sobald das Regressionsmodell geschätzt wurde, können die weiteren Voraussetzungen überprüft werden.  

### Analyse

#### __Regressionsmodell__

Die multiple Regression kann, sofern die Daten nicht hierarchisch geordnet sind, mit der Funktion `lm()` aus dem schon in R installierten Package {stats} berechnet werden.

Als Argumente benötigt die Funktion `lm()` die Modellgleichung (`av ~ uv1 + uv2`) und den Datensatz mit den Variablen (`data = ...`).
Die Resultate der Regressionsanalyse können mit der Funktion `summary()` zusammengefasst werden.

```{r}
# Einfache lineare Regression berechnen
m1 <- lm(concentration ~ sleep + sport,
      		data = d_sport)

# Output ausgeben
summary(m1)

```

#### __Konfidenzintervalle__

Konfidenzintervalle können mit der Funktion `confint()` bestimmt werden.

```{r}
# 95%-Konfidenzintervall bestimmen
confint(m1, level = 0.95)
```

Die 95%-Konfidenzintervalle zeigen beinhalten 0 sowohl für den Achsenabschnitt (_intercept_), wie auch für die Variable `sport`. Für `sleep` beinhalten die 95%-Konfidenzintervalle 0 nicht.

Es können auch 90%-Konfidenzintervalle (oder jede andere Prozentzahl) bestimmt werden:

```{r}
# 90%-Konfidenzintervall bestimmen
confint(m1, level = 0.90)
```

#### __Modellannahmen überprüfen__

Mit dem Package {performance} können die Modellannahmen einer multiplen Regression überprüft werden.

<aside> Vor dem ersten Verwenden muss das Package {performance} heruntergeladen/installiert werden, z.B. in der Konsole mit `install.packages("performance")`.</aside>

Die Funktion `check_model()` gibt diagnostische Plots aus:

```{r}
#| eval: false
performance::check_model(m1)
```
![](../imgs/multipleregression_checkmodel.jpeg)

Wie die Plots gelesen werden sollten und was getan werden kann, wenn die Annahmen nicht erfüllt sind, wird auf der [Website des Packages {performance}](https://easystats.github.io/performance/articles/check_model.html) sehr gut beschrieben.

__Linearität__

Ob die Linearität gegeben ist, kann in folgendem Plot visuell beurteilt werden. Die Referenzlinie (grün) sollte ungefähr gerade sein und horizontal liegen. Der graue Bereich sollte immer die gerade schwarze Linie beinhalten.

```{r}
diagnostic_plots <- plot(performance::check_model(m1, panel = FALSE))

# check linearity
diagnostic_plots[[2]]
```

__Homoskedastizität__

Die Homoskedastizität kann ebenfalls im oberen Plot überprüft werden.
Wenn die Streuung der Residuen um die Regressionsgerade unterschiedlich ist, dann ist die Annahme der Varianzhomogenität nicht erfüllt.
Heteroskedastizität kann beispielsweise so aussehen, dass die Residuen bei tiefen Werten von $x$ sehr wenig streuen und bei hohen Werten von $x$ stärker streuen (wie ein Trichter).

Folgender Plot zeigt die Wurzel der absoluten^[Das bedeutet, dass für ein Residuum von $-2$ der Wert $2$ verwendet würde.] Residuen (standardisiert). Weil es so keine negativen Abweichungen mehr gibt, kann in die Abweichung der Residuen eine Linie hineingelegt werden, die möglichst parallel zu $0$ sein sollte. Im Beispiel der obigen Trichterform würde dann eine ansteigende Linie zu sehen sein.

```{r}
# check homogeneity of variance
diagnostic_plots[[3]]
```

Die Homoskedastizität kann weiter mit der Funktion `check_heteroscedasticity()` überprüft werden (Breusch-Pagan test).

```{r}
performance::check_heteroscedasticity(m1)
```

__Normalverteilung der Fehlervariablen__

Die Normalverteilung der Residuen kann mit einem Q-Q-Plot überprüft werden.

```{r}
# check normality of residuals
diagnostic_plots[[6]]
```

Die Normalverteilung der Fehlervariablen kann weiter mit der Funktion `check_normality()` überprüft werden (Shapiro-Wilk test).

```{r}
performance::check_normality(m1)
```

__Outlier erkennen__

```{r}
# detect outliers
diagnostic_plots[[4]]
```


__Multikollinearität__

Wenn die Prädiktor miteinander sehr stark korrelieren wird dies Multikollinearität genannt. Dies kann zu ungenauen Parameterschätzungen führen (durch einen erhöhten Standardfehler des Regressionskoeffizienten).
Die Werte im grünen Bereich sind hier ok.

```{r}
# detect collinearity
diagnostic_plots[[5]]
```


#### __Visualisierungen__

Zur Visualisierung von Unterschieden eignen sich Punktdiagramme (_Scatterplots_) und das Einzeichnen der Regressionsgeraden (z.B. mit `geom_abline()` oder `geom_smooth()` für jedes Variablenpaar.

Möchte man den Einfluss beider Prädiktorvariablen gleichzeitig visualisieren, kann man für eine Variable eine Farbkodierung nutzen. Dies ist auch interaktiv möglich^[Verwirrenderweise ist der Regressionskoeffizient hier der erste Wert, der Achsenabschnitt kommt am Schluss.], z.B. so:

```{r}
ggiraphExtra::ggPredict(m1,interactive=TRUE)
```

<aside>[{ggiraph}-Package](https://davidgohel.github.io/ggiraph/)</aside>

In diesem Modell sind keine Interaktionen zugelassen, deshalb sind alle Linien parallel.

::: {.callout-tip appearance="default" title="Für Interessierte: Modell mit Interaktion" collapse="true"}

Soll die Interaktion von Schlaf und Sport in das Modell einbezogen werden, kann dies mit einem `*` in der Modellgleichung gekennzeichnet werden:

```{r}
m1_interaction <- lm(concentration ~ sleep * sport, 
                     data = d_sport)
```

So wird ein zusätzlicher Regressionskoeffizient für die Interaktion geschätzt:

```{r}
summary(m1_interaction)
```

Visualisiert sieht das wie folgt aus:

```{r}
ggiraphExtra::ggPredict(m1_interaction,interactive=TRUE)
```

Deskriptiv hat Schlaf immer einen positiven Einfluss auf die Konzentration. Dieser Einfluss nimmt jedoch ab, je höher der Werten für Sport (mehr Training). 
Dies wird sichtbar daran, dass der Regressionskoeffizient bei den dunkleren Werten (tieferere Sportwerte) kleiner ist, als bei den höheren.^[Verwirrenderweise ist der Regressionskoeffizient hier der erste Wert, der Achsenabschnitt kommt am Schluss.]
Inferenzstatistisch sind die Effekte von Schlaf, Sport wie auch der Interaktion nicht signifkant.

:::

Durch die Visualisierung der Rohwerte und der Regressionsgeraden, kann der "Fit", also die Passung der Geraden zu den Daten gut visuell überprüft werden.
Sichtbar wird auch, wenn die Daten nichtlineare Zusammenhänge aufweisen, die nicht mit einer linearen Regression analysiert werden können, wie beispielsweise kurvilineare Zusammenhänge.

### Schlussfolgerungen

#### __Ergebnisse interpretieren und berichten__

> Multiple regression analysis was used to test if duration of sport training and sleep significantly predicted performance in a concentration test in young adults. The results of the regression indicated the two predictors explained 35% of the variance ($R^2$ = .35, $F$(2,15) = 3.99, $p$ = .041). It was found that sleep duration significantly predicted concentration performance ($\beta_{sleep}$ = .87, $p$ = .030), whereas sport duration did not show a significant effect on concentration ($\beta_{sport}$ = 1.08, $p$ = .062).


## Weiterführende Informationen

- Interaktive Visualisierung von Zusammenhängen von [RPsychologist](https://rpsychologist.com/correlation/)
- Interaktive Visualisierung von Konfidenzintervallen von [RPsychologist](https://rpsychologist.com/d3/ci/)
- [{performance}-Package](https://easystats.github.io/performance/index.html) für Modelltests mit [`check_model()`](https://easystats.github.io/performance/articles/check_model.html)
- [{ggiraph}-Package](https://davidgohel.github.io/ggiraph/) für interaktive Visualisierungen von Modellen