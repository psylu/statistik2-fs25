# Übung 10 {.unnumbered #sec-uebung-10}

```{r}
#| include: false
# Set working directory of R
knitr::opts_knit$set(root.dir = '../')
```

::: {.callout-note appearance="default" title="Lernziele dieser Übung"}
Nach der heutigen Übung können Sie:

- _R_ und _RStudio_ zur Analyse von Daten verwenden
- Die Voraussetzungen für einfache lineare Regressionen überprüfen
- Einfache lineare Regressionen anhand der Formeln selber berechnen
- Regressionskoeffizienten und Achsenabschnitte interpretieren
:::

> Evtl. hilfreiche Visualisierungsoption bei Fragen: Magnusson, K. (2023). Interpreting Correlations: An interactive visualization (Version 0.7.1) [Web App]. R Psychologist. https://rpsychologist.com/correlation/

<br>

## Aufgabe 10.1: Voraussetzungen und deren Überprüfung 

> _[ca. 15 min]_

__a.__ Welche Voraussetzungen müssen für die einfache lineare Regression gegeben sein?

> AV ist metrisch, UV ist metrisch, (Quasi-)Linearität

__b.__ Schauen Sie sich untenstehende Punktdiagramme (_scatterplots_) an. Bei welchen Daten würden Sie eine lineare Regression berechnen, bei welchen eher nicht? (_Hinweis: Nicht alle Beispiele sind eindeutig._)

```{r}
#| message: false
#| warning: false
#| include: false

library(tidyverse)

## simulate data

## (A) AV metrisch, UV metrisch, Linearität nicht gegeben

# set parameters
n <- 100
b0 <- 10
b1 <- 0.01
b2 <- 0.00001
sigma = 5

# generate counts
sim_df <- tibble(x = runif(n, 5, 18)) |>
  mutate(mu = b0 + b1*x + b2*x^5.5)|> 
  mutate(y = rnorm(n = n, mu, sigma) )

# plot
p1 <- ggplot(data = sim_df, aes(x = x, y = y)) + 
  geom_point(size = 2.5, alpha = .2, color = "blue") +
  theme_classic() +
  labs(title = "(A) Testleistung nach Alter",
       x = "Alter in Jahren",
       y = "Testleistung in korrekten Items")

## (B) AV metrisch, UV metrisch, Linearität gegeben

# set parameters
n <- 100
b0 <- 5
b1 <- 0.4
sigma = 3

# generate counts
sim_df <- tibble(x = runif(n, 2, 20)) |>
  mutate(mu = b0 + b1*x) |> 
  mutate(y = rnorm(n = n, mu, sigma) )

# plot
p2 <- ggplot(data = sim_df, aes(x = x, y = y)) + 
  geom_point(size = 2.5, alpha = .2, color = "blue") +
  theme_classic() +
  labs(title = "(B) Laufgeschwindigkeit nach Trainingsintensität",
       x = "Trainingsintensität in Stunden pro Woche",
       y = "Laufgeschwindigkeit in kmh")

## (C) AV metrisch, UV nicht metrisch, Linearität gegeben

# set parameters
n <- 100
b0 <- 2
b1 <- 0.5
sigma = 2

# generate counts
sim_df <- tibble(x = runif(n, 1, 26)) |>
  mutate(mu = b0 + b1*x) |> 
  mutate(y = rnorm(n = n, mu, sigma) )|>
  mutate(x_int = round(x, digits = 0))

# plot
p3 <- ggplot(data = sim_df, aes(x = x_int, y = y)) + 
  geom_point(size = 2.5, alpha = .2, color = "blue") +
  theme_classic() +
  labs(title = "(C) Laufgeschwindigkeit nach Kanton",
       x = "Herkunftskanton (Identifikationsnummer)",
       y = "Laufgeschwindigkeit in kmh")

## (D) AV nicht metrisch, UV nicht metrisch, Linearität gegeben

# set parameters
n <- 100
b0 <- 55
b1 <- -2.5
sigma <- 5

# generate counts
sim_df <- tibble(x = runif(n, 5, 18)) |>
  mutate(mu = b0 + b1*x) |> 
  mutate(y = rnorm(n = n, mu, sigma) )|>
  mutate(x_int = round(x, digits = 0),
         y_int = round(y, digits = 0))

# plot
p4 <- ggplot(data = sim_df, aes(x = x_int, y = y_int)) + 
  geom_point(size = 2.5, alpha = .2, color = "blue") +
  theme_classic() +
  labs(title = "(D) Farbpräferenz je nach Studiengang",
       x = "Studiengang (codiert)",
       y = "Farbpräferenz (50 Varianten)")

## (E) AV nicht metrisch, UV metrisch, Linearität nicht gegeben

# set parameters
n <- 100
b0 <- 3
b1 <- 0.1
b2 <- 0.0001
sigma <- 1

# generate counts
sim_df <- tibble(x = runif(n, 0, 15)) |>
  mutate(mu = b0 + b1*x + b2*x^4) |> 
  mutate(y = rnorm(n = n, mu, sigma)) |>
  mutate(y_int = round(y, digits = 0))

# plot
p5 <- ggplot(data = sim_df, aes(x = x, y = y_int)) + 
  geom_point(size = 2.5, alpha = .2, color = "blue") +
  theme_classic() +
  labs(title = "(E) Trauminhalte  je nach Schlafdauer",
       x = "Schlafdauer in Stunden",
       y = "Trauminhalte")

## (F) AV metrisch, UV metrisch, Quasi-Linearität gegeben

# set parameters
n <- 100
b0 <- 70
b1 <- -1.5
b2 <- -0.001
sigma <- 5

# generate counts
sim_df <- tibble(x = runif(n, 2, 12)) |>
  mutate(mu = b0 + b1*x + b2*x^4) |> 
  mutate(y = rnorm(n = n, mu, sigma))

# plot
p6 <- ggplot(data = sim_df, aes(x = x, y)) + 
  geom_point(size = 2.5, alpha = .2, color = "blue") +
  theme_classic() +
  labs(title = "(F) Stresssymptome je nach Schlafdauer",
       x = "Schlafdauer in Stunden",
       y = "Stressymptome (Wert 0-100)")
```

```{r}
#| message: false
#| warning: false
#| include: false
library(patchwork)
p1 + p2 + p3 + p4 + p5 + p6 +
  plot_layout(widths = unit(c(7, 7), c('cm', 'cm')), heights = unit(c(7, 7, 7), c('cm', 'cm', 'cm')))
```

![Scatterplots verschiedener Datensets](../imgs/uebung-10_scatterplots.jpeg)

> (A) AV und UV metrisch, Linearität nicht gegeben.

> (B) AV metrisch, UV metrisch, Linearität gegeben.

> (C) AV metrisch, UV nicht metrisch, Linearität gegeben.

> (D) AV nicht metrisch, UV nicht metrisch, Linearität gegeben.

> (E) AV nicht metrisch (evtl. schon wenn Anzahl verschiedene Traumkategorien: kann diskutiert werden), UV metrisch, Quasi-Linearität gegeben.

> (F) AV metrisch, UV metrisch, Quasi-Linearität gegeben (kann diskutiert werden).

<br>

## Aufgabe 10.2: Komponenten der Regressionsgleichung

> _[10 min]_ 

> Hier sammeln wir alle Formeln, damit wir sie später verwenden können und dort keine Geschwindigkeitsunterschiede entstehen. Sie sollen sich kurz damit auseinandersetzen, ob sie die Formeln verstehen/Terme zuordnen können. 

> Zeitmanagement: Die Aufgabe sollte nicht länger als 15 Minuten dauern! Wichtig ist, dass sie die Formeln der Aufgaben a.-c. für die heutige Übung ready haben. Fragen zu den Formeln klären sich evtl. im Verlauf der Übung und können am Schluss nochmals aufgegriffen werden, wenn noch Zeit.

__a.__ Wie lautet die Bestimmungsgleichung der Regressionsgeraden in der einfachen linearen Regressionsanalyse? Schreiben Sie die Gleichung auf und ordnen Sie die untenstehenden Begriffe den Termen in der Gleichung zu: 

- Achsenabschnitt (_intercept_)
- Regressionsgewicht bzw. die Steigung (_slope_) der Regressionsgeraden
- Vorhersagefehler
- AV

>$\hat{y} = b_0 + b_1 * x$ 

> oder für einzelne Werte: 
> $\hat{y}_m = b_0 + b_1 * x_m$

> oder für Prädiktion mit Fehler (Simulation):
> $\hat{y}_m = b_0 + b_1 * x_m + e_m$

> $y$: AV

> $x$: UV

> $b_0$: Achsenabschnitt (_intercept_)

> $b_1$: Regressionsgewicht, Regressionskoeffizient, Steigung (_slope_)

> $e$: Residualvariable/Regressionsresiduum/Fehlerterm

__b.__ Wie lautet die Bestimmungsgleichung für $b_0$ für unstandardisierte Variablen?

> Unstandardisierter Achsenabschnitt (_intercept_): $b_0 = \bar{y} - b_1 * \bar{x}$

__c.__ Wie lautet die Bestimmungsgleichung für $b_1$ für unstandardisierte Variablen?

> Unstandardisiertes Regressionsgewicht/Steigung (_slope_): $b_1 = r_{XY} * s_y/s_x = s_{XY}/s^2_X$

__d.__ Welchen konstanten Wert hat $b_0$ bei standardisierten Variablen?

> Achsenabschnitt (_intercept_): $b_0 = 0$

__e.__ Welchem Wert entspricht $b_1$ bei standardisierten Variablen?

> Steigung (_slope_): $b_1 = r_{XY}$ (Produkt-Moment-Korrelation der Variablen)

__f.__ In welcher Einheit wird das unstandardisierte Regressionsgewicht angegeben?

> Einheit der Kriteriumsvariablen (AV). Hierbei unterscheidet sich die Regression von der Korrelation, welche zwingend in standardisierten Einheiten berichtet wird.

<br>

## Aufgabe 10.3: Anwendungsbereich 

>_[20-30 min]_

__Eigene Fragestellung bearbeiten.__

> Vorschlag: Hier sollen Sie einfach mal durcharbeiten, was sie können. Fragen dazu werden im zweiten Teil beantwortet.

__a.__ Nehmen Sie an, Sie könnten sich einen Datensatz _wünschen_. Welche für Sie spannende Forschungsfrage könnte mit einer einfachen linearen Regression analysiert werden?

__b.__ Zeichnen Sie die Daten für Ihre Forschungsfrage auf. 

- Was ist die AV und was die UV?

- Zeichnen Sie die Achsen inkl. Achsenbeschriftungen (möglicher Wertebereich) auf. Wählen Sie wenn möglich Messungen/Achsen mit positiven, ganzzahligen Werten (_integers_). Wählen Sie für $x$ (UV) eine Achse die $0$ enthält.

- Welche Daten würden Sie erwarten? Zeichnen Sie ein Punktdiagramm (_Scatterplot_) mit 10-20 Datenpunkten zu Ihrer Forschungsfrage.

__c.__ Welcher Zusammenhang besteht zwischen den beiden Variablen?

- Legen Sie eine möglichst passende (lineare) Regressionsgerade durch die Punktewolke.

- Markieren Sie den ungefähren Achsenabschnitt (_intercept_) $b_0$. Welcher Wert hat $b_0$?

- Zeichnen Sie den ungefähren Vorhersagefehler $e$ für jeden Datenpunkt ein.

- Welche Steigung (_slope_) $b_1$ hat die Gerade ungefähr? (_Tipp: Zeichnen Sie auf der Gerade 2 Punkte ein und berechnen Sie die Differenz der $x$ und $y$-Werten dieser zwei Punkte. Teilen Sie dann $dy/dx$._)

> Achsenabschnitt (_intercept_): der $y$-Wert, wo $x = 0$

> Steigung: kann berechnet werden mit $dy/dx$, wobei $dy = \hat{y}_{m+1} - \hat{y}_m$ und $dx = x_{m+1} - x_m$. 

> Fehler einzeichnen: vertikaler Strich von Datenpunkt zu Regressionsgerade


__d.__ Nutzen Sie Ihre Analyse zur Prädiktion.

- Wählen Sie einen neuen $x$-Wert.

- Berechnen Sie den neuen $y$-Wert mit der Regressionsgleichung (siehe _Aufgabe 10.2a_). Lassen Sie den Fehlerterm weg.

- Zeichnen Sie den neuen Datenpunkt ein.

<br>

__Gemeinsam eine Fragestellung bearbeiten__

__e.__ Tauschen Sie in Kleingruppen/im Plenum die Forschungsfragen aus und wählen Sie als Gruppe eine davon aus. Gehen Sie danach obige Schritte anhand diesen Beispiels durch. Sammeln Sie dieses Mal gemeinsam Daten: Jede Person zeichnet mind. einen (fiktiven) Datenpunkt ein.

Diskutieren Sie zusätzlich:

- Welche weiteren Datenpunkte würden für sehr hohe/tiefe Werte der UV vorhergesagt werden?

- Was könnte zu Fehlschätzungen führen?

> Es ist einfacher, wenn die UV auf der x-Achse liegt und die AV auf der y-Achse.

> Falls die Wolke keine Korrelation anzeigt: selber noch ein paar Punkte hinzufügen.

> Falls ein nichtlinearer Zusammenhang entsteht: Fragen, was das Problem sein könnte. Danach Datenpunkte löschen/hinzufügen für linearen Zusammenhang um fortzufahren.

> Take home: (1) Regressionen eignen sich für "Vorhersagen" je nach Fragestellung/Design muss man vorsichtig sein mit der Extrapolation, beispielsweise wenn Schlüsse ausserhalb des Datensbereichs gezogen werden (evtl. ist der Zusammenhang dort nicht mehr linear!). (2) Extremwerte können die Regression sehr stark verändern. (3) Selektionsbias in der Datenauswahl (also wenn nur ein Teil der "Punktewolke" ausgewählt/erhoben/einbezogen werden würde) kann zu Fehlern führen.

> Wenn unstandardisiert: Eine starke Steigung bedeutet NICHT eine hohe Korrelation, da die Steilheit von der Dateneinheit abhängt nicht (nur) von der Stärke des Zusammenhangs! D.h. die Stärke des Zusammenhangs kann nur aus den standardisiert aufgezeichneten Datenpunkten geschlossen werden oder wenn die Achsen dieselbe Metrik enthalten. (Oder wenn man die Einheiten inhaltlich gut versteht und die Steigung so einordnen kann.)

- Was würde passieren, wenn Sie die Achsen umdrehen? Bleiben die Werte für $b_0$ und $b_1$ gleich oder verändern diese sich?

> Die Werte verändern sich ziemlich sicher. Regressionskoeffizienten sind in der Einheit der AV angegeben. Siehe auch letzter Punkt oben. 

<br>

## Aufgabe 10.4: Kleinste Quadrate-Kriterium

> _[10 min]_ 

__a.__ Ergänzen Sie den folgenden Text zum Grundgedanken des Kleinste-Quadrate Kriteriums:

::: {.webex-check .webex-box}
"Optimale `r webexercises::mcq(c("Reduktion des Kriteriums", answer= "Vorhersage", "Bestimmung der Regressionsgeraden"))` ist gegeben, wenn `r webexercises::mcq(c("die Varianz", "das Minimum", answer="die Summe"))` der quadrierten Differenzen zwischen den aus der unabhängigen Variablen `r webexercises::mcq(c("beobachteten", answer="vorhergesagten"))` Werten der abhängigen Variablen und den `r webexercises::mcq(c(answer="beobachteten", "vorhergesagten"))` Werten der abhängigen Variablen `r webexercises::mcq(c("maximal", answer="minimal"))` ist." (Eid et al. 2017)
:::

__b.__ Diskutieren Sie diesen Gedanken kurz: Gibt es Unklarheiten? Fassen Sie den obigen Satz für Laienpersonen verständlich in 1-2 Sätzen zusammen.

> Wir legen eine gerade Linie so in die Daten, dass die insgesamte Abweichung aller Datenpunkte möglichst klein ist. Das bedeutet, dass zwar die meisten Datenpunkte nicht exakt vorhergesagt werden, aber über alle Werte gesehen der Vorhersagefehler (Prädiktionsfehler) möglichst klein ist.

## Aufgabe 10.5: Berechnen der Regressionsgeraden 

> _[20 min]_


:::{.callout-note appearance="default" title="Schlafstudie" collapse="false"}

__Beschreibung der Studie__

In der (fiktiven) Studie wurde der Einfluss von Schlafdauer bei Jugendlichen auf Konzentrationsleistung in einem neuropsychologischen Test (mögliche Scores zwischen 0 und 20) untersucht. 

__Forschungsfrage__: Hängt die nächtliche Schlafdauer mit der Konzentration am nächsten Tag zusammen?

__Daten:__ Die Grafik zeigt Messungen von 10 Jugendlichen.

:::

```{r}
#| echo: false
d <- tibble(x = c(7, 8, 5, 6, 9, 10, 9, 7, 11, 12),
            y = c(12, 12, 6, 10, 16, 18, 12, 6, 14, 18))

p1 <- d|>
  ggplot(aes(x, y)) +
  geom_point(size = 3, alpha = 0.4, color = "blue") +
  scale_x_continuous("Schlafdauer in Stunden",
                     breaks=c(0,1,2,3,4,5,6,7,8,9,10,11,12), 
                     minor_breaks = NULL,
                     limits=c(4,12)) +
  scale_y_continuous("Leistung in Konzentrationstest",
                     breaks=c(0,2,4,6,8,10,12,14,16,18,20),
                     minor_breaks = NULL,
                     limits=c(0,20)) +
  theme_minimal()
p1
```

<br>

Berechnen Sie den Regressionskoeffizienten $b_1$ und den Achsenabschnitt $b_0$ dieser Daten.

__a.__ Schreiben Sie alle Werte in eine Tabelle mit je einer Spalte für jede Variable oder geben Sie diese in _R_ ein. Benennen Sie die UV $x$ und die AV $y$.

|  $x$ | $y$  |
|---|---|
| 5 | 6 |
| 6 | 10 |
| 7 | 6 |
| 7 | 12 |
| 8 | 12 |
| 9 | 12 |
| 9 | 16 | 
| 10 | 18 |
| 11 | 14 |
| 12 | 12 |
: __Daten der Schlafstudie__ {.hover}

```{r}
#| messages: false
#| warning: false
#| eval: false
# Packages laden
library(tidyverse)

# Erstellen des Datensatzes
d <- tibble(x = c(___),
            y = c(___))
```

```{r}
#| messages: false
#| warning: false
# Packages laden
library(tidyverse)

# Erstellen des Datensatzes
d <- tibble(x = c(5, 6, 7, 7, 8, 9, 9, 10, 11, 12),
            y = c(6, 10, 6, 12, 12, 12, 16, 18, 14, 12))
```


__b.__ Berechnen Sie $r_{XY}$ (die Korrelation der beiden Variablen) und die Standardabweichungen von Hand oder mit den Funktionen `cor()` und `sd()` in _R_.

```{r}
#| eval: false
# Korrelation berechnen
r_xy <- ___(___, ___)

# Standardabweichungen berechnen
sd_x <- ___(___)
sd_y <- ___(___)
```

```{r}
# Korrelation berechnen
r_xy <- cor(d$x, d$y)

# Standardabweichungen berechnen
sd_x <- sd(d$x)
sd_y <- sd(d$y)
```

__c.__ Berechnen Sie den Regressionskoeffizienten $b_1$ von Hand oder in _R_. 

__Tipp__: Multiplizieren Sie $r_{XY}$ mit den Quotienten der Standardabweichungen von $Y$ und $X$, um das Regressionsgewicht $b_1$ zu erhalten.

```{r}
#| eval: false
# Regressionskoeffizient berechnen
b_1 <- ___ * (___/___)
```

```{r}
# Regressionskoeffizient berechnen
b_1 <- r_xy * (sd_y/sd_x)
```

__d.__ Berechnen Sie den Achsenabschnitt $b_0$. Hierfür brauchen Sie die Durchschnittswerte für $x$ und $y$.

```{r}
#| eval: false
# Achsenabschnitt berechnen
mean_x <- ___(___)
mean_y <- ___(___)
b_0 <- ___ - ___ * ___
```

```{r}
# Achsenabschnitt berechnen
mean_x <- mean(d$x)
mean_y <- mean(d$y)
b_0 <- mean_y - b_1 * mean_x

# check
#lm(y~x, data = d)
```
__e.__ Sagen Sie einen neuen Wert $y$ für eine gegebenen Wert $x$ vorher.

> Einsetzen von $x$ in die Formel.

__f.__ Fügen Sie in den Plot die Werte für die Regressionsgerade ein. (Nur möglich, wenn Sie die Daten in R eingegeben haben.)

```{r}
#| eval: false
d |>
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_abline(intercept = ___, slope = ___) +
  theme_minimal()
```

```{r}
d |>
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_abline(intercept = b_0, slope = b_1) + # oder direkt die Werte eingeben
  xlim(0, 13) +
  ylim(-5, 20) +
  theme_minimal()
```

<br>

## Aufgabe 10.6: Berechnen der Regressionsgeraden für eigenes Beispiel 

> _[15 min]_ Aufgabe kann problemlos weggelassen werden, wenn es nicht mehr reicht, da Lösung individuell und Lösungsweg oben festgehalten.

Berechnen Sie, wie in der vorherigen Aufgabe, den Regressionskoeffizienten $b_1$ und den Achsenabschnitt $b_0$ für das in _Aufgabe 10.3a-c_ selber erarbeitete Beispiel.

__a.__ Schreiben Sie die Werte in eine Tabelle mit je einer Spalte für jede Variable oder geben Sie diese in _R_ ein.

```{r}
#| messages: false
#| warning: false
#| eval: false
# Packages laden
library(tidyverse)

# Erstellen des Datensatzes
d <- tibble(x = c(___),
            y = c(___))
```

__b.__ Berechnen Sie $b_1$ und $b_0$. _Tipp_: Verwenden Sie hierfür die Funktionen `mean()`, `sd()`, `cor()` in _R_.

__c.__ Zeichnen Sie die Regressionslinie in Ihr zu Beginn erstelltes Punktediagramm ein und vergleichen Sie diese mit der zuvor von Ihnen geschätzten Regressionsgerade.
Oder plotten Sie die Daten alternativ mit {ggplot2}, siehe _Aufgabe 10.5f_.

<br>

## Zusatzaufgaben 


:::{.callout-tip appearance="default" title="Zusatzaufgaben"}

__Zusatzaufgaben müssen nicht gelöst werden.__

:::

__a.__ Berechnen Sie die Varianzen: von $y$, $\hat{y}$ und der Residuen $e$, und berechnen Sie den Determinationskoeffizienten $R^2$ und Indeterminationskoeffizienten. Was sagen diese aus?


:::{.callout-note appearance="default" collapse="true" title="Tipp"}

Erstellen Sie hierfür eine Tabelle wie in den Vorlesungsfolien und berechnen Sie die Zwischenschritte mit den Funktionen `mean()`, `sd()`, `cor()`, `var()` (siehe [Deskriptive Statistik](../datenanalyse/descriptive.html))

| $m$ | $x_m$ | $y_m$  | $\hat{y}$ | $e_m = y_m - \hat{y}_m$ |
|-------|-------|---|---|---|
| 1     |       |   |   |   |
| 2     |       |   |   |   |
| 3     |       |   |   |   |
| 4     |       |   |   |   |
| 5     |       |   |   |   |
| __*Summe*__   |   |   |   |   |
| __*M*__       |   |   |   |   |
| __*SD*__      |   |   |   |   |
| __*Varianz*__ |   | $s_{y}^2 = $  | $s_{\hat{y}}^2 =$  | $s_e^2 = $   |


:::


__b.__ Osterspecial: Lesen Sie in [diesem Artikel](https://statmodeling.stat.columbia.edu/2025/03/11/what-does-jesus-have-to-do-with-linear-regression/) was lineare Regression mit Jesus zu tun hat.


